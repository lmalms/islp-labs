{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "from ISLP import load_data\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_data(\"Boston\")\n",
    "boston.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS model\n",
    "X = pd.DataFrame(\n",
    "    {\n",
    "        \"intercept\": np.ones(boston.shape[0]),\n",
    "        \"lstat\": np.array(boston[\"lstat\"]),\n",
    "    }\n",
    ")\n",
    "y = np.array(boston[\"medv\"])\n",
    "\n",
    "model = sm.OLS(endog=y, exog=X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "X_test = pd.DataFrame(\n",
    "    {\n",
    "        \"intercept\": np.array([1., 1., 1.]),\n",
    "        \"lstat\": np.array([5., 10., 15.])\n",
    "    }\n",
    ")\n",
    "y_hat = results.get_prediction(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.predicted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence intervals\n",
    "y_hat.conf_int(alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction intervals\n",
    "y_hat.conf_int(obs=True, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "def abline(ax, b, m, *args, **kwargs):\n",
    "    \"Add a line with slope m and intercept b to ax\"\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n",
    "    ax.plot(xlim, ylim, *args, **kwargs)\n",
    "\n",
    "\n",
    "ax = boston.plot.scatter(x=\"lstat\", y=\"medv\")\n",
    "abline(\n",
    "    ax,\n",
    "    results.params[0],\n",
    "    results.params[1],\n",
    "    \"r--\",\n",
    "    lw=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(results.fittedvalues, results.resid)\n",
    "ax.set(xlabel=\"Fitted value\", ylabel=\"Residual\")\n",
    "ax.axhline(0, c=\"k\", ls=\"--\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infl = results.get_influence()\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\n",
    "ax.set(xlabel=\"Index\", ylabel=\"Levarge\")\n",
    "\n",
    "np.argmax(infl.hat_matrix_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(\n",
    "    {\n",
    "        \"intercept\": np.ones(boston.shape[0]),\n",
    "        \"lstat\": np.array(boston[\"lstat\"]),\n",
    "        \"age\": np.array(boston[\"age\"]),\n",
    "    }\n",
    ")\n",
    "y = np.array(boston[\"medv\"])\n",
    "\n",
    "model = sm.OLS(endog=y, exog=X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward selection based on p-values\n",
    "X = boston.drop(columns=[\"medv\", \"indus\", \"age\"])\n",
    "X.insert(loc=0, column=\"intercept\", value=np.ones(X.shape[0]))\n",
    "y = np.array(boston[\"medv\"])\n",
    "model = sm.OLS(endog=y, exog=X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collinearity\n",
    "vals = [VIF(X, i) for i in range(1, X.shape[1])]\n",
    "vif = pd.DataFrame({\"vif\": vals}, index=X.columns[1:])\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear and Interaction Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.drop(columns=[\"medv\", \"indus\"])\n",
    "X.insert(loc=0, column=\"intercept\", value=np.ones(X.shape[0]))\n",
    "y = np.array(boston[\"medv\"])\n",
    "model = sm.OLS(endog=y, exog=X)\n",
    "results1 = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.drop(columns=[\"medv\", \"indus\"])\n",
    "X.insert(loc=0, column=\"intercept\", value=np.ones(X.shape[0]))\n",
    "y = np.array(boston[\"medv\"])\n",
    "\n",
    "# Feature transforms\n",
    "poly_features = [\"lstat\", \"age\"]\n",
    "poly_transformer = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "col_transformer = ColumnTransformer(\n",
    "    transformers=[(\"poly\", poly_transformer, poly_features),],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "X = col_transformer.fit_transform(X, y)\n",
    "X = pd.DataFrame(X, columns=col_transformer.get_feature_names_out())\n",
    "\n",
    "model = sm.OLS(endog=y, exog=X)\n",
    "results2 = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_lm(results1, results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(results2.fittedvalues, results2.resid)\n",
    "ax.set(xlabel=\"Fitted value\", ylabel=\"Residual\")\n",
    "ax.axhline(0, c=\"k\", ls=\"--\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence vs. Prediction Intervals\n",
    "\n",
    "### Confidence Intervals for Population Estimates\n",
    "\n",
    "- The range of values that is expected to contain the true value of a population parameter, such as the population mean, with some specified level of confidence.\n",
    "- Used to quantify uncertainty in the parameter estimates of a population, e.g. the population mean.\n",
    "- Confidence intervals are computed using the standard error, the amount by which the sample estimate is expected to deviate from the the true population parameter.\n",
    "- For the sample mean this is given by:\n",
    "\n",
    "$$\n",
    "SE(\\hat\\mu) = \\frac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "- where $\\sigma$ is the **sample** standard deviation and $n$ is the sample size.\n",
    "- Confidence intervals on the sample mean are then computed using the standard error and the critical t-values for the required confidence interval.\n",
    "\n",
    "$$\n",
    "CI = \\bar{X} \\pm t_{\\alpha/2} \\cdot SE(\\hat\\mu)\n",
    "$$\n",
    "\n",
    "- for example the critical t-value for a 95% confidence interval ($\\alpha$ = 0.05) is $\\approx$ 2.05\n",
    "\n",
    "### Confidence Intervals in Regression Analysis\n",
    "\n",
    "- Instead of estimating a population parameter by sampling, we might have preformed a regression analysis.\n",
    "- For example, to estimate the height of orange trees as a function of their age we might have measured tree heights for 30, 50, 60, and 80 days old trees. We now want to estimate the average height of a 100-day old tree without going out and sampling heights for 100-day old trees.\n",
    "- Instead we can use the regression analysis to estimate the population mean height of 100-day old orange trees.\n",
    "- The confidence interval on that mean is given by\n",
    "\n",
    "$$\n",
    "CI = \\hat{Y} \\pm t_{\\alpha/2, n-2} \\cdot SE \\cdot \\sqrt{\\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{\\sum (X_i - \\bar{X})^2}}\n",
    "$$\n",
    "\n",
    "- where $SE$ is the residual standard error of the estimate, given by\n",
    "$$\n",
    "s = \\sqrt{\\frac{\\sum (Y_i - \\hat{Y}_i)^2}{n - 2}}\n",
    "$$\n",
    "\n",
    "- Note that $(X_0 - \\bar{X})$ is the difference between the point that we are making the estimate for $X_0$ and the mean value of $X$ used in the regression analysis. There the confidence interval widens as the target estimate deviates further from the sample mean. Also note that $(X_i - \\bar{X})$, captures the spread of $x$-values used in the regression analysis. More spread gives us more leverage estimate parameters.\n",
    "\n",
    "- Worth re-iterating the confidence intervals in regression analysis are about capturing the uncertainty in the population estimate where that estimate is derived from a regression analysis. Returning to the example of orange tree heights, the confidence interval will capture the uncertainty in the average height of 100-day old orange trees. Not the uncertainty in the heigh estimate for an individual tree.\n",
    "\n",
    "### Prediction Intervals.\n",
    "\n",
    "- Prediction intervals are used to capture the uncertainty in individual predictions. Prediction intervals for linear regression are given by:\n",
    "\n",
    "$$\n",
    "PI = \\hat{Y} \\pm t_{\\alpha/2, n-2} \\cdot s \\cdot \\sqrt{1 + \\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{\\sum (X_i - \\bar{X})^2}}\n",
    "$$\n",
    "\n",
    "- Comparing this against the formula of the confidence interval shows that prediction intervals are typically wider that confidence intervals to capture the additional uncertainty in making a prediction for a single value vs. a population estimate.\n",
    "\n",
    "### Summary\n",
    "\n",
    "| **Confidence Interval** | **Prediction Interval** |\n",
    "|------------------------|------------------------|\n",
    "| Used in determining population parameters based on sample statistics. | Not used in determining population parameters based on samples. |\n",
    "| Used to predict the mean response (average value of the dependent variable for a given independent variable) based on regressions. | Used to predict the future value (of an individual data point for a given independent variable) based on regressions. |\n",
    "| Usually narrower for a given analysis. | Usually wider for a given analysis. |\n",
    "\n",
    "\n",
    "### Resources\n",
    "- [Confidence vs Prediction Intervals](https://www.datacamp.com/blog/confidence-intervals-vs-prediction-intervals?dc_referrer=https%3A%2F%2Fwww.google.com%2F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
